#  Hangman AI - HMM & RL Integration

This project implements an **intelligent Hangman solver** using **Hidden Markov Models (HMMs)** and a **Reinforcement Learning (RL)** framework.  
The goal is to predict masked words efficiently based on probabilistic letter patterns learned from a text corpus.

---

##  Overview

Traditional Hangman agents guess letters based on overall frequency (e.g., â€œeâ€, â€œtâ€, â€œaâ€).  
This project goes beyond that by integrating **word-length-specific HMMs** and **transition/positional probability learning**, improving the success rate over a simple baseline.

It extends to **Q-Learning / Deep Q-Learning (DQL)** for reinforcement-based decision making.

---

## Features

 **Baseline Model:**  
Guesses letters using simple positional frequency analysis from the dataset.

 **HMM-Based Models:**  
Four different HMMs trained for:
- Short words (1â€“4 letters)
- Medium words (5â€“7 letters)
- Long words (8â€“10 letters)
- Very long words (â‰¥11 letters)

Each model learns:
- **Transition probabilities** â€” likelihood of one letter following another.
- **Positional probabilities** â€” likelihood of letters appearing at certain positions.

 **Integrated Agent:**  
Automatically chooses the right HMM model based on word length and combines it with baseline probabilities.

 **RL:**  
integrated **Q-Learning** using:
- State = current masked word + guessed letters  
- Action = next letter guess  
- Reward = correctness / progress toward solving the word

---

## Hangman RL Agent (part 2)
A reinforcement learning agent that plays Hangman by combining Q-learning with Hidden Markov Models (HMM) for intelligent letter guessing.

## Overview

Q-Learning: Learns optimal game strategy through trial and error
Combined Intelligence: Merges statistical language knowledge with learned gameplay tactics

Reward Structure
Correct guess +2 per letter revealed
Win (complete word)+50
Wrong guess-5
Game loss-20
Repeated guess-10

- Algorithm Details
Q-Learning with HMM Guidance

## Hyperparameters:

Alpha (Î±): 0.1 - Learning rate (conservative updates)
Gamma (Î³): 0.95 - Discount factor (values future rewards)
Epsilon (Îµ): 1.0 â†’ 0.01 - Exploration rate with exponential decay
Epsilon decay: 0.9995 per episode

## Training Process

Training episodes: 30,000 games
Training corpus: Variable (loaded from corpus.txt)
Test set: 2,000 words (loaded from test.txt)
Lives per game: 6 wrong guesses allowed
HMM models: Pre-trained on word length categories (short â‰¤4, medium â‰¤7, long â‰¤10, very long >10)


ðŸ“ Files Required
â”œâ”€â”€ ml_hackathon_team2.ipynb              # Main training notebook
â”œâ”€â”€ corpus.txt                # Training word list
â”œâ”€â”€ test.txt                  # Test word list (2000 words)
â”œâ”€â”€ short_hmm1.pkl            # Pre-trained HMM (â‰¤4 letters)
â”œâ”€â”€ medium_hmm1.pkl           # Pre-trained HMM (5-7 letters)
â”œâ”€â”€ long_hmm1.pkl             # Pre-trained HMM (8-10 letters)
â”œâ”€â”€ very_long_hmm1.pkl        # Pre-trained HMM (>10 letters)
â””â”€â”€ hmm_rl_agent.pkl          # Saved trained agent (output)



# Calculate score

Final Score = (Success Rate Ã— 2000) - (Wrong Guesses Ã— 5) - (Repeated Guesses Ã— 2)
